{
    "id": "8c76f2288b38ba3b",
    "type": "subflow",
    "name": "Pre-process [TF]",
    "info": "Pre-processes image for inference.\r\n\r\nSupports object detection models trained using:\r\n- Tequ TF2 training pipeline\r\n- Microsoft Custom Vision (Compact S1) \r\n\r\nhttps://github.com/Lapland-UAS-Tequ/tequ-tf2-ca-training-pipeline\r\n\r\nSupports object detection models from Tensorflow 2 Detection Model Zoo\r\n- SSD MobileNet v2 320x320\r\n- SSD MobileNet V2 FPNLite 320x320\r\n- SSD MobileNet V2 FPNLite 640x640\r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\r\n\r\nDependencies\r\n- https://www.npmjs.com/package/@tensorflow/tfjs-node-gpu",
    "category": "Tequ",
    "in": [
        {
            "x": 120,
            "y": 80,
            "wires": [
                {
                    "id": "56f3f4890f12e6e8"
                }
            ]
        }
    ],
    "out": [
        {
            "x": 500,
            "y": 80,
            "wires": [
                {
                    "id": "56f3f4890f12e6e8",
                    "port": 0
                }
            ]
        }
    ],
    "env": [
        {
            "name": "thumbnail",
            "type": "bool",
            "value": "true"
        },
        {
            "name": "width",
            "type": "num",
            "value": "85",
            "ui": {
                "type": "input",
                "opts": {
                    "types": [
                        "num",
                        "env"
                    ]
                }
            }
        },
        {
            "name": "height",
            "type": "num",
            "value": "48",
            "ui": {
                "type": "input",
                "opts": {
                    "types": [
                        "num",
                        "env"
                    ]
                }
            }
        },
        {
            "name": "model_type",
            "type": "str",
            "value": "tequ-tf2",
            "ui": {
                "type": "select",
                "opts": {
                    "opts": [
                        {
                            "l": {
                                "en-US": "Tequ TF2 Object detection"
                            },
                            "v": "tequ-tf2"
                        },
                        {
                            "l": {
                                "en-US": "Microsoft CV Object detection"
                            },
                            "v": "azure-cv-compact-s1"
                        }
                    ]
                }
            }
        }
    ],
    "meta": {
        "module": "tequ-node-red-triton-pre-process-tf",
        "type": "tequ-node-red-triton-pre-process-tf",
        "version": "0.0.1",
        "author": "juha.autioniemi@lapinamk.fi",
        "desc": "Pre-process data for NVIDIA Triton Inference Server.",
        "keywords": "tequ-ai",
        "license": "MIT"
    },
    "color": "#3FADB5",
    "inputLabels": [
        "image in"
    ],
    "outputLabels": [
        "data out"
    ],
    "icon": "node-red/swap.svg",
    "status": {
        "x": 460,
        "y": 140,
        "wires": [
            {
                "id": "7fef7fddbf14648d",
                "port": 0
            }
        ]
    },
    "flow": [
        {
            "id": "56f3f4890f12e6e8",
            "type": "function",
            "z": "8c76f2288b38ba3b",
            "name": "[TF] Image to Tensor",
            "func": "const image = msg.payload;\nlet pre_process_time;\nlet thumbnail;\nlet thumbnail_time_ms;\n\nasync function resize(inputTensor, width, height) {\n    return tf.tidy(() => {\n        let resized = tf.image.resizeBilinear(inputTensor, [height,width])\n        resized = tf.reshape(resized, [height,width,3])\n        return Promise.resolve(tf.node.encodeJpeg(resized));\n    });\n}\n\nasync function convert(input){\n    return tf.tidy(() => {\n        const tensor = tf.node.decodeJpeg(input, 3).expandDims(0);\n        const shape = tensor.shape;\n        return { \"tensor\": tensor, \"shape\": shape}\n    });\n}\n\nasync function convert_and_resize(input) {\n    return tf.tidy(() => {\n        let resized = tf.image.resizeBilinear(input, [320, 320])\n        resized = tf.reshape(resized, [1,320, 320, 3])\n        let shape = resized.shape;\n        return { \"tensor\": resized, \"shape\": shape }\n    });\n}\n\n\ntry{\n    const start = Date.now();\n    \n    let imageTensor = await convert(image);\n    let imageTensorBuffer;\n    let tensorBufferSize;\n\n    if(env.get(\"thumbnail\")){\n        const width = env.get(\"width\")\n        const height = env.get(\"height\")\n        thumbnail = Buffer.from(await resize(imageTensor[\"tensor\"], width, height))\n        msg.data.properties.object.data.original.thumbnail = thumbnail\n        thumbnail_time_ms = Date.now() - start;\n        msg.data.properties.object.data.original.thumbnail_ms = thumbnail_time_ms;\n    }\n\n    let model_type = env.get(\"model_type\");\n    let inference_header = {}\n\n    if(model_type == \"tequ-tf2\"){       \n        imageTensorBuffer = Buffer.from(imageTensor[\"tensor\"].dataSync());\n        tensorBufferSize = imageTensorBuffer.length;\n \n        inference_header = {\n            \"model_name\": \"inference\",\n            \"inputs\": [\n                {\n                    \"name\": \"input_tensor\",\n                    \"shape\": imageTensor[\"shape\"],\n                    \"datatype\": \"UINT8\",\n                    \"parameters\": {\n                        \"binary_data_size\": tensorBufferSize\n                    }\n                }\n            ],\n            \"outputs\": [\n                {\n                    \"name\": \"detection_scores\",\n                    \"parameters\": {\n                        \"binary_data\": false\n                    }\n                },\n                {\n                    \"name\": \"detection_boxes\",\n                    \"parameters\": {\n                        \"binary_data\": false\n                    }\n                },\n                {\n                    \"name\": \"detection_classes\",\n                    \"parameters\": {\n                        \"binary_data\": false\n                    }\n                }\n            ]\n        }\n    }\n    else if (model_type == \"azure-cv-compact-s1\"){\n        imageTensor = await convert_and_resize(imageTensor[\"tensor\"]);\n        imageTensorBuffer = Buffer.from(imageTensor[\"tensor\"].dataSync());\n        tensorBufferSize = imageTensorBuffer.length;\n        \n        inference_header = {\n            \"model_name\": \"inference\",\n            \"inputs\": [\n                {\n                    \"name\": \"inputs\",\n                    \"shape\": imageTensor[\"shape\"],\n                    \"datatype\": \"FP32\",\n                    \"parameters\": {\n                        \"binary_data_size\": tensorBufferSize\n                    }\n                }\n            ],\n            \"outputs\": [\n                {\n                    \"name\": \"detected_classes\",\n                    \"parameters\": {\n                        \"binary_data\": false\n                    },\n                    \"dims\":[1]\n                },\n                {\n                    \"name\": \"detected_boxes\",\n                    \"parameters\": {\n                        \"binary_data\": false\n                    },\n                    \"dims\": [4]\n                },\n                {\n                    \"name\": \"detected_scores\",\n                    \"parameters\": {\n                        \"binary_data\": false\n                    },\n                    \"dims\": [1]\n                }\n            ]\n        }\n    }\n\n    \n    const inference_header_buffer = Buffer.from(JSON.stringify(inference_header))\n    const inference_header_length = inference_header_buffer.length\n    let triton_request_data = Buffer.concat([inference_header_buffer, imageTensorBuffer])\n\n    msg.tensor = {\n        \"image\": image,\n        \"data\": triton_request_data,\n        \"tensor_shape\": imageTensor[\"shape\"],\n        \"inference_header_length\": inference_header_length,\n        \"image_tensor_buffer_length\": tensorBufferSize,\n        \"thumbnail\": thumbnail\n    }\n\n    imageTensor[\"tensor\"].dispose()\n    tf.disposeVariables()\n\n    pre_process_time = Date.now() - start;\n    msg.data.properties.computer_vision.pre_process_ms = pre_process_time\n    node.status({ fill: \"green\", shape: \"dot\", text: pre_process_time + \" ms\" });\n    return msg;\n}\ncatch (e) {\n    node.warn(e)\n    node.status({ fill: \"red\", shape: \"dot\", text: \"Convert failed...\" });\n    node.error(\"Image to tensor conversion failed. Probably input is not an image buffer.\", msg);\n}",
            "outputs": 1,
            "noerr": 0,
            "initialize": "",
            "finalize": "// Code added here will be run when the\n// node is being stopped or re-deployed.\nconst model = context.get(\"savedmodel\")\ntf.dispose(model)\ncontext.set(\"model\", undefined)\ncontext.set(\"modelInfo\", undefined)",
            "libs": [
                {
                    "var": "tf",
                    "module": "@tensorflow/tfjs-node-gpu"
                }
            ],
            "x": 280,
            "y": 80,
            "wires": [
                []
            ]
        },
        {
            "id": "7fef7fddbf14648d",
            "type": "status",
            "z": "8c76f2288b38ba3b",
            "name": "",
            "scope": null,
            "x": 320,
            "y": 140,
            "wires": [
                []
            ]
        },
        {
            "id": "147fe8b05b70ef10",
            "type": "subflow:8c76f2288b38ba3b",
            "z": "022e42e3f5d0335d",
            "name": "",
            "x": 470,
            "y": 140,
            "wires": [
                [
                    "9449dd0dfca7038a"
                ]
            ]
        }
    ]
}